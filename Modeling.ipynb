{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Bike Share Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib as plt\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "df_train['id'] = 0\n",
    "\n",
    "df_test['count'] = 0\n",
    "df_test['id'] = 1\n",
    "\n",
    "df_train = df_train.drop('casual', axis = 1)\n",
    "df_train = df_train.drop('registered', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_test = df_test.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [df_train, df_test]\n",
    "df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General data Info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17379, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>count</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  season  holiday  workingday  weather  temp   atemp  \\\n",
       "0  2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "1  2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2  2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "3  2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "4  2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "   humidity  windspeed  count  id  \n",
       "0        81        0.0     16   0  \n",
       "1        80        0.0     40   0  \n",
       "2        80        0.0     32   0  \n",
       "3        75        0.0     13   0  \n",
       "4        75        0.0      1   0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all of the variables are numeric except for the datetime column which is datatime object. We can parse this column and create new columns for the year, month, day and time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17379 entries, 0 to 6492\n",
      "Data columns (total 11 columns):\n",
      "datetime      17379 non-null object\n",
      "season        17379 non-null int64\n",
      "holiday       17379 non-null int64\n",
      "workingday    17379 non-null int64\n",
      "weather       17379 non-null int64\n",
      "temp          17379 non-null float64\n",
      "atemp         17379 non-null float64\n",
      "humidity      17379 non-null int64\n",
      "windspeed     17379 non-null float64\n",
      "count         17379 non-null int64\n",
      "id            17379 non-null int64\n",
      "dtypes: float64(3), int64(7), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# no missing variables\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling datetime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df.datetime.apply(lambda x: x.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time only gives the hour of the day (0-24), so we can remove the colon notation and just leave the hour number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = df.datetime.apply(lambda x: int(x.split()[1].split(\":\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df.date.apply(lambda x: datetime.strptime(x,'%Y-%m-%d').year)\n",
    "df['month'] = df.date.apply(lambda x: datetime.strptime(x,'%Y-%m-%d').month)\n",
    "df['day'] = df.date.apply(lambda x: datetime.strptime(x,'%Y-%m-%d').day)\n",
    "df['weekday'] = df.date.apply(lambda x: datetime.strptime(x,'%Y-%m-%d').weekday())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a feature for normal vs. odd hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Odd_hours'] = np.where((df.time >=0) & (df.time <=6), 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to change some of the nominal variables to be categories so that they do not get misinterpreted as ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nominal_variables = [\"time\",\"weekday\",\"month\",\"season\",\"weather\",\"holiday\",\"workingday\"]\n",
    "# for var in nominal_variables:\n",
    "#     df_train[var] = df_train[var].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_d = pd.DataFrame(pd.get_dummies(list(df.time), prefix = 'time_'))\n",
    "weekday_d = pd.DataFrame(pd.get_dummies(list(df.weekday), prefix='weekday_'))\n",
    "month_d = pd.DataFrame(pd.get_dummies(list(df.month), prefix='month_'))\n",
    "season_d = pd.DataFrame(pd.get_dummies(list(df.season), prefix='season_'))\n",
    "weather_d = pd.DataFrame(pd.get_dummies(list(df.weather), prefix='weather_'))\n",
    "year_d = pd.DataFrame(pd.get_dummies(list(df.year), prefix='year_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with_dummies = [weather_d, month_d, time_d, season_d, weekday_d, year_d]\n",
    "df = df.join(with_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    17379.000000\n",
       "mean       119.999770\n",
       "std        170.711941\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%         28.000000\n",
       "75%        192.000000\n",
       "max        977.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17379 entries, 0 to 10885\n",
      "Data columns (total 71 columns):\n",
      "datetime      17379 non-null object\n",
      "season        17379 non-null int64\n",
      "holiday       17379 non-null int64\n",
      "workingday    17379 non-null int64\n",
      "weather       17379 non-null int64\n",
      "temp          17379 non-null float64\n",
      "atemp         17379 non-null float64\n",
      "humidity      17379 non-null int64\n",
      "windspeed     17379 non-null float64\n",
      "count         17379 non-null int64\n",
      "id            17379 non-null int64\n",
      "date          17379 non-null object\n",
      "time          17379 non-null int64\n",
      "year          17379 non-null int64\n",
      "month         17379 non-null int64\n",
      "day           17379 non-null int64\n",
      "weekday       17379 non-null int64\n",
      "Odd_hours     17379 non-null int32\n",
      "weather__1    17379 non-null uint8\n",
      "weather__2    17379 non-null uint8\n",
      "weather__3    17379 non-null uint8\n",
      "weather__4    17379 non-null uint8\n",
      "month__1      17379 non-null uint8\n",
      "month__2      17379 non-null uint8\n",
      "month__3      17379 non-null uint8\n",
      "month__4      17379 non-null uint8\n",
      "month__5      17379 non-null uint8\n",
      "month__6      17379 non-null uint8\n",
      "month__7      17379 non-null uint8\n",
      "month__8      17379 non-null uint8\n",
      "month__9      17379 non-null uint8\n",
      "month__10     17379 non-null uint8\n",
      "month__11     17379 non-null uint8\n",
      "month__12     17379 non-null uint8\n",
      "time__0       17379 non-null uint8\n",
      "time__1       17379 non-null uint8\n",
      "time__2       17379 non-null uint8\n",
      "time__3       17379 non-null uint8\n",
      "time__4       17379 non-null uint8\n",
      "time__5       17379 non-null uint8\n",
      "time__6       17379 non-null uint8\n",
      "time__7       17379 non-null uint8\n",
      "time__8       17379 non-null uint8\n",
      "time__9       17379 non-null uint8\n",
      "time__10      17379 non-null uint8\n",
      "time__11      17379 non-null uint8\n",
      "time__12      17379 non-null uint8\n",
      "time__13      17379 non-null uint8\n",
      "time__14      17379 non-null uint8\n",
      "time__15      17379 non-null uint8\n",
      "time__16      17379 non-null uint8\n",
      "time__17      17379 non-null uint8\n",
      "time__18      17379 non-null uint8\n",
      "time__19      17379 non-null uint8\n",
      "time__20      17379 non-null uint8\n",
      "time__21      17379 non-null uint8\n",
      "time__22      17379 non-null uint8\n",
      "time__23      17379 non-null uint8\n",
      "season__1     17379 non-null uint8\n",
      "season__2     17379 non-null uint8\n",
      "season__3     17379 non-null uint8\n",
      "season__4     17379 non-null uint8\n",
      "weekday__0    17379 non-null uint8\n",
      "weekday__1    17379 non-null uint8\n",
      "weekday__2    17379 non-null uint8\n",
      "weekday__3    17379 non-null uint8\n",
      "weekday__4    17379 non-null uint8\n",
      "weekday__5    17379 non-null uint8\n",
      "weekday__6    17379 non-null uint8\n",
      "year__2011    17379 non-null uint8\n",
      "year__2012    17379 non-null uint8\n",
      "dtypes: float64(3), int32(1), int64(12), object(2), uint8(53)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'count', 'id', 'date', 'time', 'year',\n",
       "       'month', 'day', 'weekday', 'Odd_hours', 'weather__1', 'weather__2',\n",
       "       'weather__3', 'weather__4', 'month__1', 'month__2', 'month__3',\n",
       "       'month__4', 'month__5', 'month__6', 'month__7', 'month__8', 'month__9',\n",
       "       'month__10', 'month__11', 'month__12', 'time__0', 'time__1', 'time__2',\n",
       "       'time__3', 'time__4', 'time__5', 'time__6', 'time__7', 'time__8',\n",
       "       'time__9', 'time__10', 'time__11', 'time__12', 'time__13', 'time__14',\n",
       "       'time__15', 'time__16', 'time__17', 'time__18', 'time__19', 'time__20',\n",
       "       'time__21', 'time__22', 'time__23', 'season__1', 'season__2',\n",
       "       'season__3', 'season__4', 'weekday__0', 'weekday__1', 'weekday__2',\n",
       "       'weekday__3', 'weekday__4', 'weekday__5', 'weekday__6', 'year__2011',\n",
       "       'year__2012'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since temp and atemp are so highly correlated, we can drop temp from the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmsle(y_true, y_preds):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_preds))\n",
    "\n",
    "# Defining 'coz mean_squared_log_error removed from scikit-learn ver. 0.2\n",
    "def rmsle_2(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.log(y_true) - np.log(y_pred))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove cols**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['id'] == 0]\n",
    "df_test = df[df['id'] == 1]\n",
    "\n",
    "df_test = df_test.drop('count', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop('count', axis=1)\n",
    "y = df_train['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Regression - Base Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2012-05-19 20:00:00'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-027b30d0ce76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m--> 458\u001b[1;33m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    757\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;31m# make sure we actually converted to numeric:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"O\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '2012-05-19 20:00:00'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because linear regression can predict negative values, we cannot calcualte the RMLSE for the predictions, and this is thus not a valid method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rmsle(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'rmsle', float(np.sqrt(mean_squared_log_error(preds, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "def run_xgb(param):\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dval = xgb.DMatrix(X_val, y_val)\n",
    "    dtest = xgb.DMatrix(X_test, y_test)\n",
    "    \n",
    "    num_round = 5000\n",
    "    watchlist = [(dval, 'eval'), (dtrain, 'train')]\n",
    "    evals_result = {}\n",
    "\n",
    "    bst = xgb.train(param, dtrain, num_round, evals=watchlist, early_stopping_rounds=10, \n",
    "                    evals_result=evals_result, feval=eval_rmsle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf = rf.fit(X_train, y_train)\n",
    "preds = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selector = [\n",
    "    \n",
    "    'id',\n",
    "    'count',\n",
    "    \n",
    "    'atemp', \n",
    "    'windspeed',\n",
    "    'humidity',\n",
    "    'weather',\n",
    "    \n",
    "    'season__1',\n",
    "    'season__2',\n",
    "    'season__3',\n",
    "    'season__4'\n",
    "\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = df[feature_selector]\n",
    "\n",
    "df_train = my_df[my_df['id'] == 0]\n",
    "df_test = my_df[my_df['id'] == 1]\n",
    "\n",
    "df_test = df_test.drop('count', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'silent':1, \n",
    "    'objective':'count:poisson', \n",
    "}\n",
    "\n",
    "run_xgb(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = evals_result['train']['rmsle']\n",
    "val_error = evals_result['eval']['rmsle']\n",
    "df_error = pd.DataFrame([train_error, val_error]).T\n",
    "df_error.columns = ['train', 'val']\n",
    "\n",
    "df_error.plot(title=\"XGBoost learning curves\", ylim=(0,.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bst.predict(dtest, ntree_limit = bst.best_ntree_limit)\n",
    "labels = dtest.get_label()\n",
    "rmsle(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Test: 0.44**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest_final = xgb.DMatrix(df_test)\n",
    "\n",
    "preds = bst.predict(dtest_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.Series(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.Series(datetime_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.concat([dt, preds], axis=1)\n",
    "submission = submission.rename(index=str, columns={\"datetime\": \"datetime\", 0: \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(r'Submissions/submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data to match required format\n",
    "## Copy data frame to separate different model workflows\n",
    "df_rf = df\n",
    "\n",
    "# Make list of columns to be removed\n",
    "## Columns for which dummy var is created\n",
    "remove_dummy_cols_rf = ['weather', 'month', 'time', 'season', 'year', 'weekday']\n",
    "## Other columns to be removed\n",
    "remove_other_cols_rf = ['temp','datetime', 'date', 'holiday', 'day']\n",
    "## Merging columns to be removed\n",
    "remove_cols_rf = remove_dummy_cols_rf + remove_other_cols_rf\n",
    "df_rf = df_rf.drop(remove_cols_rf, axis = 1)\n",
    "# df_rf.columns\n",
    "\n",
    "# Separate global test and train\n",
    "df_train_rf = df_rf[df_rf['id'] == 0]\n",
    "df_test_rf = df_rf[df_rf['id'] == 1]\n",
    "df_test_rf = df_test_rf.drop('count', axis=1)\n",
    "\n",
    "#Separate Training set into X (features) and y(targets)\n",
    "X_rf = df_train_rf.drop(['count','id'], axis=1)\n",
    "y_rf = df_train_rf['count']\n",
    "\n",
    "## Split training set in train and test\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y_rf, test_size=0.2)\n",
    "## Further split train set into train and validation\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(X_train_rf, y_train_rf, test_size=0.2)\n",
    "\n",
    "# Convert every pandas df to nparray\n",
    "X_train_rf = np.array(X_train_rf.values.tolist())\n",
    "X_test_rf = np.array(X_test_rf.values.tolist())\n",
    "X_val_rf = np.array(X_val_rf.values.tolist())\n",
    "y_train_rf = np.array(y_train_rf.values.tolist())\n",
    "y_test_rf = np.array(y_test_rf.values.tolist())\n",
    "y_val_rf = np.array(y_val_rf.values.tolist())\n",
    "\n",
    "# Preparing global test matrix\n",
    "test_rf = df_test_rf.drop(['id'], axis=1)\n",
    "test_rf = np.array(test_rf.values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 26, 'min_samples_split': 2, 'n_estimators': 70}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############CAUTION - Following takes ~7Mins to run##############\n",
    "# Grid Search for Random Forest Model\n",
    "param_grid = { \n",
    "                'n_estimators': [40, 50, 60, 70],\n",
    "                'max_depth' : list(range(20,30,2)),\n",
    "                'min_samples_split' : list(range(2,12,2))\n",
    "            }\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rf_regr, \n",
    "                      param_grid=param_grid,\n",
    "                      cv= 5,\n",
    "                      scoring='neg_mean_squared_log_error')\n",
    "CV_rfc.fit(X_train_rf, y_train_rf)\n",
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 97, 'min_samples_split': 3, 'n_estimators': 40}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomized Search\n",
    "param_grid = { \n",
    "                'n_estimators': randint(40,100),\n",
    "                'max_depth' : randint(10,100),\n",
    "                'min_samples_split' : randint(2,10)\n",
    "            }\n",
    "\n",
    "CV_rfc = RandomizedSearchCV(estimator=rf_regr,\n",
    "                            n_iter=5,\n",
    "                            param_distributions=param_grid,\n",
    "                            cv= 5,\n",
    "                            scoring='neg_mean_squared_log_error')\n",
    "CV_rfc.fit(X_train_rf, y_train_rf)\n",
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=26,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=70, n_jobs=None,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "\n",
    "## Initialize Regressor\n",
    "rf_regr = RandomForestRegressor(n_estimators=70,\n",
    "                                max_depth=26, \n",
    "                                random_state=0,\n",
    "                                min_samples_split=2,\n",
    "                               criterion='mse')\n",
    "rf_regr.fit(X_train_rf, y_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error:0.2020 \n",
      "Validation error:0.4045 \n",
      "Test error:0.4262\n"
     ]
    }
   ],
   "source": [
    "# Train error\n",
    "rf_train_predictions = rf_regr.predict(X_train_rf)\n",
    "rf_train_error = rmsle_2(y_train_rf,rf_train_predictions)\n",
    "rf_train_error\n",
    "\n",
    "# Validation error\n",
    "rf_val_predictions = rf_regr.predict(X_val_rf)\n",
    "rf_val_error = rmsle_2(y_val_rf,rf_val_predictions)\n",
    "rf_val_error\n",
    "\n",
    "# Test error\n",
    "rf_test_predictions = rf_regr.predict(X_test_rf)\n",
    "rf_test_error = rmsle_2(y_test_rf,rf_test_predictions)\n",
    "rf_test_error\n",
    "\n",
    "print('Train error:{:.4f} \\nValidation error:{:.4f} \\nTest error:{:.4f}'.format(rf_train_error, rf_val_error, rf_test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test on submission set\n",
    "rf_submission_predictions = rf_regr.predict(test_rf)\n",
    "preds = pd.Series(rf_submission_predictions)\n",
    "dt = pd.Series(datetime_test)\n",
    "\n",
    "# Generate submission file\n",
    "submission = pd.concat([dt, preds], axis=1)\n",
    "submission = submission.rename(index=str, columns={\"datetime\": \"datetime\", 0: \"count\"})\n",
    "time_str = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "file_name = 'Submissions/submission_' + time_str + '.csv'\n",
    "submission.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
